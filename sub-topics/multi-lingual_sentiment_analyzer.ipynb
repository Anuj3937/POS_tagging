{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abafd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Install required libraries\n",
    "%pip install transformers datasets nltk spacy langdetect textblob googletrans==4.0.0-rc1 matplotlib seaborn scikit-learn ipywidgets\n",
    "%python -m spacy download en_core_web_sm\n",
    "%python -m spacy download de_core_news_sm\n",
    "%python -m nltk.downloader punkt stopwords vader_lexicon\n",
    "\n",
    "# Step 2: Import all necessary libraries\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "import ipywidgets as widgets\n",
    "from googletrans import Translator\n",
    "\n",
    "# Load spaCy models\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "nlp_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Step 3: Define all necessary functions\n",
    "\n",
    "# Function for language detection\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        # For simplicity, map all languages to either 'en' or 'de'\n",
    "        if lang == 'en':\n",
    "            return 'en'\n",
    "        elif lang == 'de':\n",
    "            return 'de'\n",
    "        else:\n",
    "            return 'en'  # Default to English for other languages\n",
    "    except:\n",
    "        return \"en\"  # Default to English if detection fails\n",
    "\n",
    "# Function for tokenization and normalization\n",
    "def preprocess_text(text, language):\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(text)\n",
    "    else:\n",
    "        # Fallback to NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "        return [token.lower() for token in tokens if token.isalpha()]\n",
    "\n",
    "    # Using spaCy for preprocessing\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return tokens\n",
    "\n",
    "# Function for named entity recognition\n",
    "def extract_entities(text, language):\n",
    "    if language == 'en':\n",
    "        doc = nlp_en(text)\n",
    "    elif language == 'de':\n",
    "        doc = nlp_de(text)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Simple entity linking (using spaCy NER)\n",
    "def link_entities(text, language):\n",
    "    entities = extract_entities(text, language)\n",
    "    # In a real project, you would link these to Wikidata or another KB\n",
    "    # For simplicity, we'll just return the entities\n",
    "    return entities\n",
    "\n",
    "# Simple sentiment analysis using TextBlob\n",
    "def analyze_sentiment_textblob(text, language):\n",
    "    try:\n",
    "        if language == 'en':\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "        elif language == 'de':\n",
    "            # For German, just use TextBlob (not TextBlobDE to avoid extra dependencies)\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "        return 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Setup sentiment analyzer using transformers\n",
    "def setup_sentiment_analyzer():\n",
    "    try:\n",
    "        # Load multilingual sentiment analysis model\n",
    "        model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "        return sentiment_analyzer\n",
    "    except:\n",
    "        # Fallback function if transformer model fails\n",
    "        def fallback_analyzer(text):\n",
    "            polarity = TextBlob(text).sentiment.polarity\n",
    "            if polarity > 0.1:\n",
    "                return [{'label': 'POSITIVE', 'score': polarity}]\n",
    "            elif polarity < -0.1:\n",
    "                return [{'label': 'NEGATIVE', 'score': abs(polarity)}]\n",
    "            else:\n",
    "                return [{'label': 'NEUTRAL', 'score': 0.5}]\n",
    "        return fallback_analyzer\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "sentiment_analyzer = setup_sentiment_analyzer()\n",
    "\n",
    "# Simple bias detection based on word frequencies\n",
    "def detect_bias(texts, language, bias_words=None):\n",
    "    # Default bias words (simplified example)\n",
    "    if bias_words is None:\n",
    "        if language == 'en':\n",
    "            bias_words = {\n",
    "                'gender': ['he', 'she', 'man', 'woman', 'boy', 'girl'],\n",
    "                'race': ['black', 'white', 'asian', 'hispanic'],\n",
    "                'age': ['young', 'old', 'elderly', 'teen']\n",
    "            }\n",
    "        elif language == 'de':\n",
    "            bias_words = {\n",
    "                'gender': ['er', 'sie', 'mann', 'frau', 'junge', 'mädchen'],\n",
    "                'race': ['schwarz', 'weiß', 'asiatisch'],\n",
    "                'age': ['jung', 'alt', 'ältere', 'jugendlich']\n",
    "            }\n",
    "\n",
    "    # Create a document-term matrix\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(lowercase=True)\n",
    "        dtm = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Check for bias words\n",
    "        bias_scores = {}\n",
    "        for category, words in bias_words.items():\n",
    "            category_score = 0\n",
    "            for word in words:\n",
    "                if word in feature_names:\n",
    "                    word_idx = np.where(feature_names == word)[0]\n",
    "                    if len(word_idx) > 0:\n",
    "                        category_score += dtm[:, word_idx[0]].sum()\n",
    "            bias_scores[category] = category_score\n",
    "    except:\n",
    "        # Fallback if vectorization fails\n",
    "        bias_scores = {'gender': 0, 'race': 0, 'age': 0}\n",
    "\n",
    "    return bias_scores\n",
    "\n",
    "# Initialize translator\n",
    "translator = Translator()\n",
    "\n",
    "# Function to translate text\n",
    "def translate_text(text, source_lang, target_lang):\n",
    "    try:\n",
    "        translation = translator.translate(text, src=source_lang, dest=target_lang)\n",
    "        return translation.text\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return \"Translation failed\"\n",
    "\n",
    "# Function for simple translation quality assessment\n",
    "def assess_translation_quality(original, translation, target_lang):\n",
    "    try:\n",
    "        # 1. Length ratio (very simple metric)\n",
    "        length_ratio = len(translation) / len(original) if len(original) > 0 else 0\n",
    "\n",
    "        # 2. Entity preservation (check if entities are preserved)\n",
    "        original_entities = extract_entities(original, 'en' if target_lang == 'de' else 'de')\n",
    "        translation_entities = extract_entities(translation, target_lang)\n",
    "\n",
    "        entity_preservation = len(translation_entities) / len(original_entities) if len(original_entities) > 0 else 1\n",
    "\n",
    "        # 3. Sentiment preservation\n",
    "        original_sentiment = analyze_sentiment_textblob(original, 'en' if target_lang == 'de' else 'de')\n",
    "        translation_sentiment = analyze_sentiment_textblob(translation, target_lang)\n",
    "\n",
    "        sentiment_diff = abs(original_sentiment - translation_sentiment)\n",
    "\n",
    "        # Combine metrics (simple average)\n",
    "        quality_score = (1 - abs(1 - length_ratio) + entity_preservation + (1 - sentiment_diff)) / 3\n",
    "\n",
    "        return {\n",
    "            'length_ratio': length_ratio,\n",
    "            'entity_preservation': entity_preservation,\n",
    "            'sentiment_difference': sentiment_diff,\n",
    "            'overall_quality': quality_score\n",
    "        }\n",
    "    except:\n",
    "        # Fallback if assessment fails\n",
    "        return {\n",
    "            'length_ratio': 0.5,\n",
    "            'entity_preservation': 0.5,\n",
    "            'sentiment_difference': 0.5,\n",
    "            'overall_quality': 0.5\n",
    "        }\n",
    "\n",
    "# Main pipeline function\n",
    "def cultural_analysis_pipeline(text):\n",
    "    try:\n",
    "        # Step 1: Language detection\n",
    "        language = detect_language(text)\n",
    "        print(f\"Detected language: {language}\")\n",
    "\n",
    "        # Step 2: Preprocessing\n",
    "        tokens = preprocess_text(text, language)\n",
    "        print(f\"Preprocessed tokens: {tokens[:10]}...\")\n",
    "\n",
    "        # Step 3: Sentiment analysis\n",
    "        sentiment_tb = analyze_sentiment_textblob(text, language)\n",
    "        sentiment_tf = sentiment_analyzer(text)[0]\n",
    "        print(f\"Sentiment (TextBlob): {sentiment_tb}\")\n",
    "        print(f\"Sentiment (Transformer): {sentiment_tf}\")\n",
    "\n",
    "        # Step 4: Entity extraction and linking\n",
    "        entities = link_entities(text, language)\n",
    "        print(f\"Entities: {entities}\")\n",
    "\n",
    "        # Step 5: Bias detection\n",
    "        bias_scores = detect_bias([text], language)\n",
    "        print(f\"Bias scores: {bias_scores}\")\n",
    "\n",
    "        # Step 6: Translation (if needed)\n",
    "        if language == 'en':\n",
    "            translation = translate_text(text, 'en', 'de')\n",
    "            target_lang = 'de'\n",
    "        else:\n",
    "            translation = translate_text(text, language, 'en')\n",
    "            target_lang = 'en'\n",
    "\n",
    "        print(f\"Translation: {translation}\")\n",
    "\n",
    "        # Step 7: Translation quality assessment\n",
    "        quality = assess_translation_quality(text, translation, target_lang)\n",
    "        print(f\"Translation quality: {quality}\")\n",
    "\n",
    "        # Return comprehensive results\n",
    "        return {\n",
    "            'language': language,\n",
    "            'tokens': tokens,\n",
    "            'sentiment': {\n",
    "                'textblob': sentiment_tb,\n",
    "                'transformer': sentiment_tf\n",
    "            },\n",
    "            'entities': entities,\n",
    "            'bias': bias_scores,\n",
    "            'translation': {\n",
    "                'text': translation,\n",
    "                'quality': quality\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in pipeline: {e}\")\n",
    "        # Return a default result structure if the pipeline fails\n",
    "        return {\n",
    "            'language': 'unknown',\n",
    "            'tokens': [],\n",
    "            'sentiment': {\n",
    "                'textblob': 0,\n",
    "                'transformer': {'label': 'NEUTRAL', 'score': 0.5}\n",
    "            },\n",
    "            'entities': [],\n",
    "            'bias': {'gender': 0, 'race': 0, 'age': 0},\n",
    "            'translation': {\n",
    "                'text': 'Translation failed',\n",
    "                'quality': {\n",
    "                    'length_ratio': 0,\n",
    "                    'entity_preservation': 0,\n",
    "                    'sentiment_difference': 0,\n",
    "                    'overall_quality': 0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create the web interface\n",
    "def create_simple_ui():\n",
    "    # Create input widgets\n",
    "    text_input = widgets.Textarea(\n",
    "        value='Enter text to analyze',\n",
    "        placeholder='Type something',\n",
    "        description='Text:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='100%', height='100px')\n",
    "    )\n",
    "\n",
    "    analyze_button = widgets.Button(\n",
    "        description='Analyze Text',\n",
    "        disabled=False,\n",
    "        button_style='success',\n",
    "        tooltip='Click to analyze',\n",
    "        icon='check'\n",
    "    )\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Define button click behavior\n",
    "    def on_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            print(\"Analyzing text...\")\n",
    "            results = cultural_analysis_pipeline(text_input.value)\n",
    "\n",
    "            # Display results\n",
    "            print(f\"Language: {results['language']}\")\n",
    "            print(f\"Sentiment (TextBlob): {results['sentiment']['textblob']}\")\n",
    "            print(f\"Sentiment (Transformer): {results['sentiment']['transformer']}\")\n",
    "            print(f\"Entities: {results['entities']}\")\n",
    "            print(f\"Bias scores: {results['bias']}\")\n",
    "            print(f\"Translation: {results['translation']['text']}\")\n",
    "            print(f\"Translation quality: {results['translation']['quality']}\")\n",
    "\n",
    "    analyze_button.on_click(on_button_clicked)\n",
    "\n",
    "    # Combine widgets and display\n",
    "    display(HTML('Cultural Analysis Tool'))\n",
    "    display(text_input, analyze_button, output)\n",
    "\n",
    "# Call the function to create and display the UI\n",
    "create_simple_ui()\n",
    "\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
