{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "Based on your input, here's information from https://en.wikipedia.org/wiki/Natural_language_processing:\n",
      "Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Source: https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "Based on your input, here's information from https://en.wikipedia.org/wiki/Natural_language_processing:\n",
      "Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Source: https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "Based on your input, here's information from https://en.wikipedia.org/wiki/Natural_language_processing:\n",
      "Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Source: https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "Based on your input, here's information from https://en.wikipedia.org/wiki/Natural_language_processing:\n",
      "Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n",
      "\n",
      "Source: https://en.wikipedia.org/wiki/Natural_language_processing\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "No relevant information found on this topic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "No relevant information found on this topic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "No relevant information found on this topic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "No relevant information found on this topic.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Thank you for using the text similarity detection system!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Preprocess texts\n",
    "    processed_text1 = preprocess_text(text1)\n",
    "    processed_text2 = preprocess_text(text2)\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([processed_text1, processed_text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_score = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    return similarity_score\n",
    "\n",
    "def generate_response(user_input, reference_database):\n",
    "    max_similarity = 0\n",
    "    best_match = None\n",
    "    \n",
    "    for entry in reference_database:\n",
    "        similarity = calculate_similarity(user_input, entry['text'])\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_match = entry\n",
    "    \n",
    "    if max_similarity > 0.1:  \n",
    "        response = f\"Based on your input, here's information from {best_match['website']}:\\n\"\n",
    "        response += best_match['text']\n",
    "        response += f\"\\n\\nSource: {best_match['website']}\"\n",
    "        return response\n",
    "    else:\n",
    "        return \"No relevant information found on this topic.\"\n",
    "\n",
    "# Sample reference database\n",
    "reference_database = [\n",
    "    {\n",
    "        'text': \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\",\n",
    "        'website': \"https://www.sas.com/en_us/insights/analytics/machine-learning.html\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development.\",\n",
    "        'website': \"https://www.python.org/doc/essays/blurb/\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\",\n",
    "        'website': \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Main program loop\n",
    "while True:\n",
    "    user_input = input(\"Enter your query (or 'quit' to exit): \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    response = generate_response(user_input, reference_database)\n",
    "    print(\"\\nResponse:\")\n",
    "    print(response)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Thank you for using the text similarity detection system!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Similarity Detection System\n",
      "--------------------------------\n",
      "This system will find relevant information based on your query.\n",
      "Type 'quit' to exit the program.\n",
      "\n",
      "\n",
      "Response:\n",
      "Based on your input, here's information from https://www.python.org/doc/essays/blurb/:\n",
      "Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development.\n",
      "\n",
      "Source: https://www.python.org/doc/essays/blurb/\n",
      "(Similarity score: 1.00)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "No relevant information found on this topic. (Best match score: 0.00)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Response:\n",
      "No relevant information found on this topic. (Best match score: 0.00)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Thank you for using the text similarity detection system!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Preprocess texts\n",
    "    processed_text1 = preprocess_text(text1)\n",
    "    processed_text2 = preprocess_text(text2)\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if not processed_text1 or not processed_text2:\n",
    "        return 0\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform([processed_text1, processed_text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    set1 = set(processed_text1.split())\n",
    "    set2 = set(processed_text2.split())\n",
    "    \n",
    "    if not set1 or not set2:\n",
    "        jaccard_sim = 0\n",
    "    else:\n",
    "        jaccard_sim = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    \n",
    "    # Calculate fuzzy ratio\n",
    "    fuzzy_ratio = fuzz.token_set_ratio(processed_text1, processed_text2) / 100\n",
    "    \n",
    "    # Return the maximum of cosine, Jaccard, and fuzzy similarities\n",
    "    return max(cosine_sim, jaccard_sim, fuzzy_ratio)\n",
    "\n",
    "def generate_response(user_input, reference_database):\n",
    "    max_similarity = 0\n",
    "    best_match = None\n",
    "    \n",
    "    for entry in reference_database:\n",
    "        similarity = calculate_similarity(user_input, entry['text'])\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_match = entry\n",
    "    \n",
    "    if max_similarity > 0.1:  # Lower threshold for better matching\n",
    "        response = f\"Based on your input, here's information from {best_match['website']}:\\n\"\n",
    "        response += best_match['text']\n",
    "        response += f\"\\n\\nSource: {best_match['website']}\"\n",
    "        response += f\"\\n(Similarity score: {max_similarity:.2f})\"\n",
    "        return response\n",
    "    else:\n",
    "        return f\"No relevant information found on this topic. (Best match score: {max_similarity:.2f})\"\n",
    "\n",
    "# Expanded reference database\n",
    "reference_database = [\n",
    "    {\n",
    "        'text': \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\",\n",
    "        'website': \"https://www.sas.com/en_us/insights/analytics/machine-learning.html\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Python is an interpreted, object-oriented, high-level programming language with dynamic semantics. Its high-level built in data structures, combined with dynamic typing and dynamic binding, make it very attractive for Rapid Application Development.\",\n",
    "        'website': \"https://www.python.org/doc/essays/blurb/\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Natural Language Processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\",\n",
    "        'website': \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Machine learning is a subset of artificial intelligence that focuses on the development of algorithms and statistical models that enable computer systems to improve their performance on a specific task through experience.\",\n",
    "        'website': \"https://www.ibm.com/cloud/learn/machine-learning\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"Python programming language is widely used in data science, web development, automation, scientific computing, and artificial intelligence applications. It's known for its readability and simplicity.\",\n",
    "        'website': \"https://www.python.org/about/\"\n",
    "    },\n",
    "    {\n",
    "        'text': \"NLP techniques are used in various applications including chatbots, sentiment analysis, language translation, text summarization, and speech recognition systems.\",\n",
    "        'website': \"https://www.datarobot.com/blog/what-is-natural-language-processing-nlp/\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Main program loop\n",
    "def main():\n",
    "    print(\"Text Similarity Detection System\")\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"This system will find relevant information based on your query.\")\n",
    "    print(\"Type 'quit' to exit the program.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter your query: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        response = generate_response(user_input, reference_database)\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"Thank you for using the text similarity detection system!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seleniumNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading selenium-4.30.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.10)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.9.0)\n",
      "Downloading selenium-4.30.0-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.4 MB 1.1 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.1/9.4 MB 1.0 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.2/9.4 MB 1.4 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/9.4 MB 1.5 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/9.4 MB 1.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.5/9.4 MB 1.5 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.6/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.7/9.4 MB 1.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.8/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.9/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.0/9.4 MB 1.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.1/9.4 MB 1.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.2/9.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 1.4/9.4 MB 1.8 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.5/9.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.6/9.4 MB 1.9 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.7/9.4 MB 1.9 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 1.9/9.4 MB 2.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.0/9.4 MB 2.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.1/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.2/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.5/9.4 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.6/9.4 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.8/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.0/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.2/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.3/9.4 MB 2.4 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.4/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.5/9.4 MB 2.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.6/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.7/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.7/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.8/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.0/9.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.1/9.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.1/9.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.2/9.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.4/9.4 MB 2.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 4.6/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.7/9.4 MB 2.3 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.8/9.4 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.0/9.4 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.1/9.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.4/9.4 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.6/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.7/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.9/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.0/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.1/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.2/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.3/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.4/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.6/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.6/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.7/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.9/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.9/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.0/9.4 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 7.1/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.2/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.4/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.6/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.7/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.8/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.9/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.3/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.4/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.5/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.6/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.8/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.9/9.4 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.0/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.1/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.4 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "   ---------------------------------------- 0.0/492.9 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/492.9 kB 991.0 kB/s eta 0:00:01\n",
      "   -------- ------------------------------- 102.4/492.9 kB 1.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 153.6/492.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 225.3/492.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 286.7/492.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 368.6/492.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  481.3/492.9 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 492.9/492.9 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB 1.7 MB/s eta 0:00:00\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.3.0 outcome-1.3.0.post0 selenium-4.30.0 trio-0.29.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web-Based Text Similarity Detection System\n",
      "------------------------------------------\n",
      "This system will search the web for information based on your query.\n",
      "Type 'quit' to exit the program.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Searching the web for relevant information...\n",
      "\n",
      "Response:\n",
      "No relevant information found on the web for your query.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Searching the web for relevant information...\n",
      "\n",
      "Response:\n",
      "Based on your input, here's information from en.wikipedia.org:\n",
      "\n",
      "A bank is a financial institution that accepts deposits from the public and creates a demand deposit while simultaneously making loans, mobilizing saver surplus to deficit spenders.[1] Lending activities can be directly performed by the bank or indirectly through capital markets.[2] Whereas banks play an important role in financial stability and the economy of a country, most jurisdictions exercise a high degree of regulation over banks. Most countries have institutionalized a system known as fractional-reserve banking, under which banks hold liquid assets equal to only a portion of their current liabilities.[3] In addition to other regulations intended to ensure liquidity, banks are generally subject to minimum capital requirements based on an international set of capital standards, the Basel Accords.[4]\n",
      "\n",
      "Source: https://en.wikipedia.org/wiki/bank\n",
      "Citation: Bank - Wikipedia. (None). Retrieved from en.wikipedia.org\n",
      "(Similarity score: 1.00)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Thank you for using the web-based text similarity detection system!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "    return ' '.join(processed_tokens)\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    # Preprocess texts\n",
    "    processed_text1 = preprocess_text(text1)\n",
    "    processed_text2 = preprocess_text(text2)\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # Handle empty strings\n",
    "    if not processed_text1 or not processed_text2:\n",
    "        return 0\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform([processed_text1, processed_text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    set1 = set(processed_text1.split())\n",
    "    set2 = set(processed_text2.split())\n",
    "    \n",
    "    if not set1 or not set2:\n",
    "        jaccard_sim = 0\n",
    "    else:\n",
    "        jaccard_sim = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    \n",
    "    # Calculate fuzzy ratio\n",
    "    fuzzy_ratio = fuzz.token_set_ratio(processed_text1, processed_text2) / 100\n",
    "    \n",
    "    # Return the maximum of cosine, Jaccard, and fuzzy similarities\n",
    "    return max(cosine_sim, jaccard_sim, fuzzy_ratio)\n",
    "\n",
    "def search_web(query, num_results=5):\n",
    "    \"\"\"\n",
    "    Search the web for information related to the query\n",
    "    \"\"\"\n",
    "    # For a real implementation, you might use a search API like Google Custom Search or Bing Search\n",
    "    # Here we'll use a simple approach by directly scraping some educational websites\n",
    "    \n",
    "    # Define a list of websites to search\n",
    "    websites = [\n",
    "        f\"https://en.wikipedia.org/wiki/{query.replace(' ', '_')}\",\n",
    "        f\"https://www.britannica.com/search?query={query.replace(' ', '+')}\",\n",
    "        f\"https://www.sciencedirect.com/search?qs={query.replace(' ', '+')}\",\n",
    "        f\"https://scholar.google.com/scholar?q={query.replace(' ', '+')}\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Define headers to mimic a browser\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    for url in websites:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                # Parse the HTML content\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Extract the title\n",
    "                title = soup.title.string if soup.title else \"No title found\"\n",
    "                \n",
    "                # Extract main content (this varies by website)\n",
    "                content = \"\"\n",
    "                \n",
    "                # For Wikipedia\n",
    "                if \"wikipedia.org\" in url:\n",
    "                    paragraphs = soup.select(\"div.mw-parser-output p\")\n",
    "                    content = \" \".join([p.get_text() for p in paragraphs[:3]])\n",
    "                \n",
    "                # For other websites, try to get main content\n",
    "                else:\n",
    "                    # Look for paragraphs\n",
    "                    paragraphs = soup.find_all('p')\n",
    "                    content = \" \".join([p.get_text() for p in paragraphs[:5]])\n",
    "                \n",
    "                # Clean up the content\n",
    "                content = re.sub(r'\\s+', ' ', content).strip()\n",
    "                \n",
    "                # Get the domain for citation\n",
    "                domain = urlparse(url).netloc\n",
    "                \n",
    "                # Add to results if content was found\n",
    "                if content and len(content) > 100:\n",
    "                    results.append({\n",
    "                        'text': content[:1000],  # Limit content length\n",
    "                        'website': url,\n",
    "                        'title': title,\n",
    "                        'domain': domain\n",
    "                    })\n",
    "                    \n",
    "                    # Stop once we have enough results\n",
    "                    if len(results) >= num_results:\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_citation_info(url):\n",
    "    \"\"\"\n",
    "    Extract citation information from a webpage\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract basic citation information\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        # Try to find author information\n",
    "        author = None\n",
    "        author_meta = soup.find('meta', {'name': ['author', 'Author']})\n",
    "        if author_meta:\n",
    "            author = author_meta.get('content')\n",
    "        \n",
    "        # Try to find publication date\n",
    "        date = None\n",
    "        date_meta = soup.find('meta', {'name': ['date', 'pubdate', 'publishdate', 'publication_date', 'article:published_time']})\n",
    "        if date_meta:\n",
    "            date = date_meta.get('content')\n",
    "        \n",
    "        # Get the domain\n",
    "        domain = urlparse(url).netloc\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'date': date,\n",
    "            'domain': domain,\n",
    "            'url': url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting citation from {url}: {e}\")\n",
    "        return {\n",
    "            'title': \"Unable to retrieve title\",\n",
    "            'author': None,\n",
    "            'date': None,\n",
    "            'domain': urlparse(url).netloc,\n",
    "            'url': url\n",
    "        }\n",
    "\n",
    "def format_citation(citation_info):\n",
    "    \"\"\"\n",
    "    Format citation information in APA style\n",
    "    \"\"\"\n",
    "    title = citation_info.get('title', 'No title')\n",
    "    author = citation_info.get('author', 'No author')\n",
    "    date = citation_info.get('date', 'n.d.')\n",
    "    domain = citation_info.get('domain', '')\n",
    "    url = citation_info.get('url', '')\n",
    "    \n",
    "    # Format the date\n",
    "    if date and len(date) > 4:\n",
    "        try:\n",
    "            # Try to extract year\n",
    "            year = re.search(r'20\\d{2}|19\\d{2}', date).group(0)\n",
    "            date = year\n",
    "        except:\n",
    "            date = 'n.d.'\n",
    "    \n",
    "    # Format the citation in APA style\n",
    "    if author and author != 'No author':\n",
    "        citation = f\"{author}. ({date}). {title}. Retrieved from {domain}\"\n",
    "    else:\n",
    "        citation = f\"{title}. ({date}). Retrieved from {domain}\"\n",
    "    \n",
    "    return citation\n",
    "\n",
    "def generate_web_response(user_input):\n",
    "    \"\"\"\n",
    "    Generate a response based on web search results\n",
    "    \"\"\"\n",
    "    # Search the web for information\n",
    "    search_results = search_web(user_input)\n",
    "    \n",
    "    if not search_results:\n",
    "        return \"No relevant information found on the web for your query.\"\n",
    "    \n",
    "    # Find the most relevant result\n",
    "    max_similarity = 0\n",
    "    best_match = None\n",
    "    \n",
    "    for result in search_results:\n",
    "        similarity = calculate_similarity(user_input, result['text'])\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_match = result\n",
    "    \n",
    "    if max_similarity > 0.1:\n",
    "        # Extract citation information\n",
    "        citation_info = extract_citation_info(best_match['website'])\n",
    "        \n",
    "        # Format the citation\n",
    "        citation = format_citation(citation_info)\n",
    "        \n",
    "        # Generate the response\n",
    "        response = f\"Based on your input, here's information from {best_match['domain']}:\\n\\n\"\n",
    "        response += best_match['text']\n",
    "        response += f\"\\n\\nSource: {best_match['website']}\"\n",
    "        response += f\"\\nCitation: {citation}\"\n",
    "        response += f\"\\n(Similarity score: {max_similarity:.2f})\"\n",
    "        \n",
    "        return response\n",
    "    else:\n",
    "        return f\"No highly relevant information found on the web. (Best match score: {max_similarity:.2f})\"\n",
    "\n",
    "# Main program loop\n",
    "def main():\n",
    "    print(\"Web-Based Text Similarity Detection System\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"This system will search the web for information based on your query.\")\n",
    "    print(\"Type 'quit' to exit the program.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter your query: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        print(\"\\nSearching the web for relevant information...\")\n",
    "        response = generate_web_response(user_input)\n",
    "        print(\"\\nResponse:\")\n",
    "        print(response)\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    print(\"Thank you for using the web-based text similarity detection system!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
